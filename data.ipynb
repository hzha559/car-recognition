{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import cv2\n",
    "from skimage import io\n",
    "import fs\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "import multiprocessing\n",
    "#import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "cuda = torch.cuda.is_available()\n",
    "import time\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import torch.nn.functional as F\\nfrom torch.autograd import Variable\\n\\ndef one_hot(index, classes):\\n    size = index.size() + (classes,)\\n    view = index.size() + (1,)\\n\\n    mask = torch.Tensor(*size).fill_(0)\\n    index = index.view(*view)\\n    ones = 1.\\n\\n    if isinstance(index, Variable):\\n        ones = Variable(torch.Tensor(index.size()).fill_(1))\\n        mask = Variable(mask, volatile=index.volatile)\\n\\n    return mask.scatter_(1, index, ones)\\n\\n\\nclass FocalLoss(nn.Module):\\n\\n    def __init__(self, gamma=0, eps=1e-7):\\n        super(FocalLoss, self).__init__()\\n        self.gamma = gamma\\n        self.eps = eps\\n\\n    def forward(self, input, target):\\n        y = one_hot(target, input.size(-1))\\n        logit = F.softmax(input, dim=-1)\\n        logit = logit.clamp(self.eps, 1. - self.eps)\\n\\n        loss = -1 * y * torch.log(logit) # cross entropy\\n        loss = loss * (1 - logit) ** self.gamma # focal loss\\n\\n        return loss.sum()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def one_hot(index, classes):\n",
    "    size = index.size() + (classes,)\n",
    "    view = index.size() + (1,)\n",
    "\n",
    "    mask = torch.Tensor(*size).fill_(0)\n",
    "    index = index.view(*view)\n",
    "    ones = 1.\n",
    "\n",
    "    if isinstance(index, Variable):\n",
    "        ones = Variable(torch.Tensor(index.size()).fill_(1))\n",
    "        mask = Variable(mask, volatile=index.volatile)\n",
    "\n",
    "    return mask.scatter_(1, index, ones)\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=0, eps=1e-7):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        y = one_hot(target, input.size(-1))\n",
    "        logit = F.softmax(input, dim=-1)\n",
    "        logit = logit.clamp(self.eps, 1. - self.eps)\n",
    "\n",
    "        loss = -1 * y * torch.log(logit) # cross entropy\n",
    "        loss = loss * (1 - logit) ** self.gamma # focal loss\n",
    "\n",
    "        return loss.sum()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#check all the testing images, if they all have 3 color channels\\ntest_path=\\'C:/Users/zhaoh/Downloads/test image/\\'\\nfor f in fs.open_fs(test_path).walk.files(filter=[\"*.jpg\"]):\\n    name=os.path.join(test_path+str(f))\\n    #print(name)\\n    a=io.imread(name)\\n    if a.shape[2] != 3:\\n        print(name)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#check all the testing images, if they all have 3 color channels\n",
    "test_path='C:/Users/zhaoh/Downloads/test image/'\n",
    "for f in fs.open_fs(test_path).walk.files(filter=[\"*.jpg\"]):\n",
    "    name=os.path.join(test_path+str(f))\n",
    "    #print(name)\n",
    "    a=io.imread(name)\n",
    "    if a.shape[2] != 3:\n",
    "        print(name)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename all the training pictures to avoid strange file name\n",
    "'''\n",
    "test_path = 'C:/Users/zhaoh/Downloads/test image/'\n",
    "i=0\n",
    "for f in fs.open_fs(test_path).walk.files(filter=[\"*.jpg\"]):\n",
    "    name=os.path.join(test_path+str(f))\n",
    "    #print(name)\n",
    "        #print(name[0:44])\n",
    "\n",
    "    os.rename(name,name[0:45]+str(i)+'.jpg')\n",
    "    i=i+1\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_dir=\"C:/Users/zhaoh/Downloads/car images/\"\\ni=0\\nfor p in fs.open_fs(data_dir).walk.files(filter=[\"*.jpg\"]):\\n    filename=data_dir+p\\n    #if \\'远景\\' in filename and not \\'远景X\\'in filename:\\n    #if (\"奥迪A4\" or \"奥迪A3\" or \"奥迪A6\" or \"奥迪A8\" or \"奥迪A7\") in filename:\\n    if \\'沃尔沃S\\' in filename or (\\'沃尔沃XC\\' in filename and \\'沃尔沃亚太\\' in filename) or \\'沃尔沃V40\\' in filename or \\'沃尔沃V60\\' in filename or \\'沃尔沃V90\\' in filename:\\n                index=80\\n                #self.data_ids.append(index)\\n                #self.data_filenames.append(filename)\\n                #a[index]=a[index]+1#沃尔沃\\n        #i=i+1\\nprint(i)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''data_dir=\"C:/Users/zhaoh/Downloads/car images/\"\n",
    "i=0\n",
    "for p in fs.open_fs(data_dir).walk.files(filter=[\"*.jpg\"]):\n",
    "    filename=data_dir+p\n",
    "    #if '远景' in filename and not '远景X'in filename:\n",
    "    #if (\"奥迪A4\" or \"奥迪A3\" or \"奥迪A6\" or \"奥迪A8\" or \"奥迪A7\") in filename:\n",
    "    if '沃尔沃S' in filename or ('沃尔沃XC' in filename and '沃尔沃亚太' in filename) or '沃尔沃V40' in filename or '沃尔沃V60' in filename or '沃尔沃V90' in filename:\n",
    "                index=80\n",
    "                #self.data_ids.append(index)\n",
    "                #self.data_filenames.append(filename)\n",
    "                #a[index]=a[index]+1#沃尔沃\n",
    "        #i=i+1\n",
    "print(i)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#车型字典：品牌名称，文件名过滤关键字，训练集编号（如果默认训练集编号直接使用enumerate的index）\n",
    "dictionary={'奥迪轿车':[\"奥迪A4L\",\"奥迪A3\",\"奥迪A6L\",'奥迪A8',\"奥迪A7\",0 ],\n",
    "            '奥迪SUV':[\"奥迪Q2\",\"奥迪Q3\",\"奥迪Q5\" ,\"奥迪Q7\",\"奥迪Q8\",1],\n",
    "            #'奥迪跑车':[\"奥迪TT/\",\"奥迪A5\",2],#\n",
    "            '奔驰轿车':['奔驰A级',\"奔驰C级\" ,\"奔驰E级\" , \"奔驰S级\",3],\n",
    "            '奔驰SUV':[\"奔驰GL\" ,\"奔驰G级\" , \"奔驰M级\" , \"奔驰EQC\",4],\n",
    "            #'奔驰跑车':[\"奔驰SL\" ,\"奔驰CL\" ,\"AMG GT\",5],\n",
    "            '奔驰厢式车':['福建奔驰',6],\n",
    "            '宝马轿车':[\"宝马1系\" ,'宝马2系旅行车',\"宝马3系\" ,\"宝马5系\" ,\"宝马7系\",7],\n",
    "            '宝马SUV':[\"宝马X1\" ,\"宝马X2\" ,\"宝马X3\" ,\"宝马X4\", \"宝马X5\",\"宝马X6\",\"宝马X7\",8],\n",
    "            #'宝马跑车':[\"宝马8系\" ,\"宝马6系\" , \"宝马i8\" ,\"宝马Z4\",9],#\n",
    "            '本田轿车':[\"飞度\",\"凌派\",\"锋范\" ,\"雅阁\" ,\"INSPIRE\" ,\"享域\", \"思铂睿\" , \"思域\" , \"哥瑞\" , \"竞瑞\",10 ],\n",
    "            '本田SUV':[\"歌诗图\",\"缤智\",\"皓影\" ,\"冠道\" ,\"本田CR-V\", \"本田UR-V\" , \"本田XR-V\",11 ],\n",
    "            '本田MPV':[\"艾力绅\",\"奥德赛\" ,\"杰德\" ,12],\n",
    "            '别克轿车':[\"VELITE 5\" , \"君威\" ,\"君越\" ,\"林荫大道\" , \"威朗\" , \"英朗\" , \"阅朗\" , \"别克微蓝\" ,\"VERANO\",13 ],\n",
    "            '别克SUV':['昂科',14],\n",
    "            '别克MPV':['别克GL6', '别克GL8',15],#别克mpv#########以上全部检查完毕###########\n",
    "            '比亚迪轿车':[\"速锐\" ,\"比亚迪e\" ,\"秦\" , \"比亚迪F\" ,\"比亚迪G\",16],#\n",
    "            '比亚迪SUV':[\"元\" ,\"宋\" , \"唐\" ,\"比亚迪S\",17],\n",
    "            #'比亚迪MPV':['比亚迪M',0],\n",
    "            '宝骏':['宝骏E','宝骏3','宝骏5','宝骏7',18],\n",
    "            #'保时捷':[\"Panamera\" , \"保时捷911\" ,\"保时捷718\" , \"918 Spyder\" , \"Boxster\" , \"Carrera GT\" , \"Cayenne\" , \"Macan\" , \"Taycan\" ,19],\n",
    "            '标致轿车':[\"标致206\" ,\"标致207\" , \"标致301\" , \"标致307\", \"标致308\" , \"标致408\",\"标致508\",20],\n",
    "            '标致SUV':[\"标致2008\" , \"标致3008\" , \"标致4008\" , \"标致5008\",21],\n",
    "            '北京轿车':[\"绅宝D\"  ,\"BEIJING-U7\",22],\n",
    "            '北京SUV':[\"北京BJ90\",\"北京BJ20\",\"北京BJ40\",\"北京BJ80\",\"绅宝X\" , \"智行\",\"BEIJING-X3\",23],\n",
    "            '宝沃SUV':[\"宝沃BX\",24],\n",
    "            #'宾利':[\"慕尚\" , \"欧陆\" ,\"飞驰\" ,\"添越\",25],\n",
    "            '奔腾轿车':['奔腾B',26],\n",
    "            '奔腾SUV':[\"奔腾X40\" ,\"奔腾X80\" , \"奔腾T\",27],\n",
    "            '北汽SUV':[\"战旗\" ,  \"BJ 212\" , \"北京BW007\" ,\"勇士\" , \"北汽幻速S\" ,\"北汽新能源EX\",28],\n",
    "            '北汽MPV':[\"北汽幻速H\" , \"北汽昌河M\" , \"北汽威望M\" , \"北汽瑞丽\",29],\n",
    "            '北汽轿车':[\"北汽新能源EC\" , \"北汽新能源EU\" , \"北斗星\",30],\n",
    "            '北汽皮卡':[\"越铃\" , \"锐铃\" ,\"福瑞达\",31],\n",
    "            '长安轿车':[\"奔奔\" , \"悦翔\" , \"逸动\" , \"锐程\" , \"睿驰\",32],\n",
    "            '长安SUV':[\"长安CS\" , \"长安新能源E-Pro\" , \"凌轩\" ,\"长安欧尚\",33],\n",
    "            '长安厢式车':[\"长安V\" ,\"长安之星\" , \"睿行\",34],\n",
    "            '长安皮卡':[\"神骐\" , \"长安凯程F70\" , \"长安星卡L1\" , \"跨越王\" ,\"新豹MINI\",35],\n",
    "            '长城皮卡':[\"长城风骏\" , \"炮\",36],\n",
    "            '大众轿车':[\"Polo\" , \"桑塔纳\" , \"朗逸\" ,\"帕萨特\" , \"辉昂\" , \"宝来\" , \"高尔夫\" , \"迈腾\" , \"速腾\" , \"捷达\",37],\n",
    "            '大众SUV':[\"C-TREK蔚领\" , \"探影\" , \"探歌\" , \"探岳\" , \"蔚揽\" ,\"途锐\" , \"途观\" , \"途铠\" ,\"途昂\",38],\n",
    "            '大众MPV':[\"迈特威\" , \"夏朗\",39],\n",
    "            '东风SUV':[\"风行T\" , \"风神AX\" , \"风光\" , \"东风风度MX6\" , \"锐骐多功能车\" , \"奥丁\",40],\n",
    "            '东风厢型车':[\"东风小康\" ,\"帅客\" , \"俊风\" ,\"御风\",41],\n",
    "            '东风轿车':[\"东风风神A60\" , \"东风风神E70\" , \"东风风神L60\" , \"奕炫\" , \"俊风E\" , \"富康ES500\",42],\n",
    "            '东风皮卡':[\"御风P16\" , \"锐骐皮卡\" , \"俊风\",43],\n",
    "            #'道奇SUV':[\"酷威\",44],\n",
    "            '东南轿车':[\"菱悦\" ,\"东南A5翼舞\" ,\"东南DX\",45],\n",
    "            '丰田轿车':[\"YARiS L\" ,\"雷凌\" , \"威驰\" ,\"卡罗拉\",46],\n",
    "            '丰田SUV':[\"丰田C-HR\" , \"汉兰达\" ,\"奕泽IZOA\" ,\"RAV4荣放\" ,\"普拉多\" , \"兰德酷路泽\" ,\"FJ 酷路泽\",47],\n",
    "            '丰田MPV':[\"埃尔法\" ,\"威尔法\" , \"普瑞维亚\",48],\n",
    "            #'丰田客车':[\"HIACE\", \"柯斯达\",49],\n",
    "            '福特轿车':[\"福克斯\",\"福睿斯\",\"金牛座\",50],\n",
    "            '福特SUV':[\"翼搏\" , \"锐际\", \"翼虎\",\"领界\" , \"撼路者\",\"探险者\",\"Ranger\",51],\n",
    "            #'福特跑车':['Mustang',52],\n",
    "            '福特MPV':[\"全顺\" , \"途睿欧\",53],\n",
    "            #'福田MPV':[\"伽途GT\",\"风景V5\",\"蒙派克E\",\"风景G5\",\"伽途i\" , \"萨瓦纳\",54],\n",
    "            #'福田厢式车':[\"风景G7\",\"风景G9\",\"图雅诺\",\"蒙派克S\",55],\n",
    "            #'福田皮卡':[\"祥菱V\" ,\"拓陆者\",56],\n",
    "            #'法拉利跑车':[\"法拉利\",57],\n",
    "            '广汽传祺轿车':[\"传祺GA\" , '广汽丰田iA5','祺智EV' ,'Aion S(埃安S)',58],\n",
    "            '广汽传祺SUV':[\"传祺GS\" ,'广汽ix4' ,'祺智PHEV' ,'传祺GE3','Aion LX(埃安LX)',59],\n",
    "            '广汽传祺MPV':[\"传祺GM\",60],\n",
    "            '观致汽车':[\"观致3\" , \"观致5\",61],\n",
    "            #'GMC汽车':[\"YUKON\" ,\"SAVANA\" , \"SIERRA\",62],\n",
    "            '长城SUV':[\"哈弗H\" ,\"哈弗F\",63],\n",
    "            #'红旗':['红旗',64],\n",
    "            '海马轿车':[\"海马爱尚EV\",\"海马E3\",\"海马M3\",\"福美来\",65],\n",
    "            '海马SUV':[\"海马8S\",\"海马S\",66],\n",
    "            #'金龙厢式车':[\"海格H\",67],\n",
    "            '吉利轿车':[\"金刚\",\"缤瑞\",\"帝豪/\",\"帝豪GL\",\"远景/\",\"博瑞\",'嘉际新能源',68],\n",
    "            '吉利SUV':['嘉际','吉利icon','远景X','博越','缤越','帝豪GS',\"远景X\",69],\n",
    "            '吉普SUV':['广汽菲克Jeep','大切诺基(进口)','自由人','自由光','自由侠','自由客','牧马人','指南者','指挥官经典',70],\n",
    "            '捷达汽车':[\"捷达V\",71],\n",
    "            '江淮家用车':['江淮iEV','嘉悦A5/','瑞风S',72],\n",
    "            '江淮商用车':['凌铃','星锐','江淮V7','帅铃T','瑞风M','瑞风',73],\n",
    "            '金杯':['金杯F50','阁瑞斯','金杯大海狮','金杯海狮','金杯快运','小海狮X','海星T','金杯T30',74],\n",
    "            #'捷豹':['捷豹X' ,'捷豹F', '捷豹I-PACE','捷豹E-PACE',75] ,\n",
    "            '捷途':['捷途X',76],\n",
    "            '凯迪拉克轿车':[\"凯迪拉克ATS-L\",'凯迪拉克CT', '凯迪拉克XTS','凯迪拉克SLS',77],\n",
    "            '凯迪拉克suv':[\"凯迪拉克XT\" , '凯雷德','凯雷德ESCALADE',78],\n",
    "            #‘克莱斯勒’:[\"克莱斯勒300C\" ,'大捷龙',79],\n",
    "            '雷克萨斯轿车':[\"雷克萨斯CT\" , '雷克萨斯IS','雷克萨斯ES', '雷克萨斯LS','雷克萨斯LC', '雷克萨斯RC','雷克萨斯GS',80],\n",
    "            '雷克萨斯suv':[\"雷克萨斯UX\",'雷克萨斯NX','雷克萨斯RX','雷克萨斯LX',81],\n",
    "            '路虎suv':['路虎',0],\n",
    "            #'林肯':['林肯',0],\n",
    "            '领克':['领克',0],\n",
    "            '铃木':['昌河铃木','雨燕','天语 SX4',0],\n",
    "            '雷诺':['东风雷诺','Espace',0],\n",
    "            #'兰博基尼':['Urus','Hurac','Aventador','Gallardo',0],\n",
    "            #'劳斯莱斯':['幻影', '古思特','库里南', '魅影','曜影',0],\n",
    "            '猎豹':['猎豹CS',0],\n",
    "            '马自达轿车':['马自达3','阿特兹','睿翼','马自达CX',0],\n",
    "            '名爵':['名爵6', '名爵3', '名爵7', '锐行' , '名爵ZS','名爵HS',0],\n",
    "            #'迷你':['MINI O','MINI CLUBMAN','MINI COUNTRYMAN',0],\n",
    "            #'玛莎拉蒂':['Ghibli','总裁','Levante','GranTurismo','GranCabrio',0],\n",
    "            '纳智捷suv':['大7 SUV',0],\n",
    "            '奇瑞轿车':['艾瑞泽','旗云','风云',0],\n",
    "            '奇瑞suv':['瑞虎',0],\n",
    "            '起亚轿车':['KX CROSS','起亚K','焕驰','福瑞迪' ,'凯绅', '秀尔',0],\n",
    "            '起亚suv':['起亚KX' ,'智跑',0],\n",
    "            '启辰suv':['启辰D60', '启辰e30' ,'启辰T60', '启辰T70', '启辰T90', '启辰M50V',0],\n",
    "            '日产轿车':['骊威','骐达TIIDA','轩逸' ,'阳光' ,'蓝鸟','天籁','西玛',0],\n",
    "            '日产suv':['劲客','逍客','途达','奇骏','楼兰',0],\n",
    "            '日产厢式车':['日产NV200','纳瓦拉','日产D22',0],\n",
    "            '荣威':['荣威3','荣威i', '荣威RX','荣威MARVEL X',0],\n",
    "            '斯柯达轿车':['晶锐','明锐','昕动','昕锐','速派','柯米克','柯珞克','柯迪亚克',0],\n",
    "            '三菱':['蓝瑟' , '翼神','广汽三菱','帕杰罗(进口)',0],\n",
    "            '上汽大通':['上汽MAXUS D', '上汽MAXUS T', '上汽MAXUS G',0],\n",
    "            '斯巴鲁':['力狮','斯巴鲁XV','森林人','斯巴鲁BRZ','傲虎',0],\n",
    "            #'Smart':['smart fortwo' ,'smart forfour',0],\n",
    "            'SWM斯威汽车':['SWM斯威汽车',0],\n",
    "            '特斯拉':['Model 3','Model S','Model X',0],\n",
    "            '沃尔沃':['沃尔沃S','沃尔沃亚太/沃尔沃XC','沃尔沃V40','沃尔沃V60','沃尔沃V90',0],\n",
    "            '五菱厢式车':['五菱宏光','五菱荣光V','五菱之光/','五菱之光V',0],\n",
    "            '五菱皮卡':['五菱荣光小卡','五菱荣光新卡','五菱之光小卡',0],\n",
    "            'WEY SUV':['WEY',0],\n",
    "            #'五十铃':[]\n",
    "            '蔚来汽车':['蔚来ES',0],\n",
    "            '现代轿车':['瑞纳','悦纳','北京现代i30','悦动','菲斯塔','领动','伊兰特EV','逸行','名图','索纳塔九','索纳塔插电混动',0],\n",
    "            '现代suv':['昂希诺','北京现代ix','途胜','胜达',0],\n",
    "            '雪佛兰轿车':['迈锐宝','科鲁','科沃兹','迈锐宝','赛欧','科迈罗',0],\n",
    "            '雪佛兰suv':['科帕奇','创酷','创界','探界者',0],\n",
    "            '雪铁龙':['雪铁龙C','爱丽舍','C4世嘉','云逸 C4 AIRCROSS','天逸 C5 AIRCROSS','C4 PICASSO',0],\n",
    "            '新宝骏':['新宝骏RC-6','新宝骏RS-3','新宝骏RS-5','新宝骏RM-5',0],\n",
    "            '英菲尼迪':['英菲尼迪Q50','英菲尼迪QX50','英菲尼迪QX80','英菲尼迪FX',0],\n",
    "            #'依维柯':['依维柯',0],\n",
    "            '天津一汽':['威志V5','夏利N','骏派',0],\n",
    "            '众泰':['众泰Z','众泰T','众泰SR','大迈X','众泰E200',0],\n",
    "            '中华':['中华H','中华V','中华骏捷F','中华骏捷',0]\n",
    "           }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for index, item in enumerate(dictionary.items()):\\n    print(index,item)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for index, item in enumerate(dictionary.items()):\n",
    "    print(index,item)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class DrivingDataset(Dataset):\n",
    "    def __init__(self,data_dir, input_w=224, input_h=224,is_train=True,transform=None):\n",
    "        \"\"\"{0:\"other\",1:\"drink\",2:\"phone\",3:\"smoke\"}\"\"\"\n",
    "        self.data_filenames = []\n",
    "        self.data_ids = []\n",
    "        self.is_train=is_train\n",
    "\n",
    "        self.data_root=fs.open_fs(data_dir)\n",
    "        self.transform = transform\n",
    "        index=0\n",
    "        \n",
    "        countlist = [[0,''] for i in range(120)]\n",
    "        #count the number in each category\n",
    "        for p in self.data_root.walk.files(filter=[\"*.jpg\"]):\n",
    "            filename=data_dir+p\n",
    "#########################################a            \n",
    "            for index,item in enumerate(dictionary.items()):\n",
    "                #print(index.dtype)\n",
    "                for i in range (len(item[1])-1):\n",
    "                    if item[1][i] in filename:#i每个车型\n",
    "                        #print(countlist[index])\n",
    "                        countlist[index][0]+=1\n",
    "                        self.data_ids.append(index)\n",
    "                        self.data_filenames.append(filename)\n",
    "                        \n",
    "             \n",
    "            \n",
    "            \n",
    "  \n",
    "        # print(self.data_filenames)\n",
    "        #print(self.data_ids)\n",
    "        \n",
    "        print('total number of training images',len(self.data_ids))\n",
    "        for index,item in enumerate(dictionary.items()):\n",
    "            countlist[index][1]=item[0]\n",
    "        print(countlist)\n",
    "                \n",
    "        \n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"Grey(i, j) = 0.299 × R(i, j) + 0.587 × G(i, j) + 0.114 × B(i, j)\"\"\"\n",
    "\n",
    "        img_path = self.data_filenames[item]\n",
    "        #print(img_path)\n",
    "        target = self.data_ids[item]\n",
    "\n",
    "        image = io.imread(img_path)#发现cv2不支持路径中有中文，换成io.imread\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        #resize_image = self.resize_padding(image, self.input_w, self.input_h)\n",
    "        #transpose_image = resize_image[:, :, ::-1].copy()\n",
    "        \n",
    "        #img = self.transform(transpose_image)\n",
    "        #print(img.shape)\n",
    "        target = np.array([target], dtype=np.long)#########################\n",
    "        target = torch.from_numpy(target)\n",
    "        return image,target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_filenames)\n",
    "    \n",
    "    def resize_padding(self, x, inputwidth, inputheight):\n",
    "        scale = float(float(inputwidth) / float(inputheight))\n",
    "        h, w, c = x.shape\n",
    "        if float(float(w) / float(h)) >= scale:\n",
    "            # padding h\n",
    "            dif = np.abs(w / scale - h)\n",
    "            pad1, pad2 = int(dif // 2), int(dif - dif // 2)\n",
    "            pad = ((pad1, pad2), (0, 0), (0, 0))\n",
    "            input_x = np.pad(x, pad, \"constant\", constant_values=0)\n",
    "            input_img = cv2.resize(input_x, (inputwidth, inputheight))\n",
    "        else:\n",
    "            # padding w\n",
    "            dif = np.abs(h * scale - w)\n",
    "            pad1, pad2 = int(dif // 2), int(dif - dif // 2)\n",
    "            pad = ((0, 0), (pad1, pad2), (0, 0))\n",
    "            input_x = np.pad(x, pad, \"constant\", constant_values=0)\n",
    "            input_img = cv2.resize(input_x, (inputwidth, inputheight))\n",
    "\n",
    "        return input_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad=np.random.randint(25,size=1).item()\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            #transforms.Pad(np.random.randint(25,size=1).item(), fill=255, padding_mode='constant'),\n",
    "            transforms.Resize((224,224), interpolation=2),\n",
    "            #transforms.RandomRotation(15),\n",
    "            transforms.RandomResizedCrop((224,224), scale=(0.7, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2),    \n",
    "            transforms.RandomHorizontalFlip(),   \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#count the number of training images##################################only for test\n",
    "\n",
    "#test_path = 'C:/Users/zhaoh/Downloads/car recognition/data/honda/sedan/'\n",
    "test_path='C:/Users/zhaoh/Downloads/car images/'\n",
    "i=0\n",
    "#carlabel=[[0 for x in range(10000)] for x in range(2)] \n",
    "#print(carlabel)\n",
    "data_root=fs.open_fs(test_path)\n",
    "for p in data_root.walk.files(filter=[\"*.jpg\"]):\n",
    "    i=i+1\n",
    "\n",
    "print(i)\n",
    "##############################################\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of training images 58680\n",
      "[[596, '奥迪轿车'], [1181, '奥迪SUV'], [1024, '奔驰轿车'], [1456, '奔驰SUV'], [180, '奔驰厢式车'], [1244, '宝马轿车'], [886, '宝马SUV'], [1491, '本田轿车'], [936, '本田SUV'], [202, '本田MPV'], [519, '别克轿车'], [259, '别克SUV'], [120, '别克MPV'], [1232, '比亚迪轿车'], [1693, '比亚迪SUV'], [1036, '宝骏'], [855, '标致轿车'], [389, '标致SUV'], [206, '北京轿车'], [509, '北京SUV'], [61, '宝沃SUV'], [34, '奔腾轿车'], [240, '奔腾SUV'], [454, '北汽SUV'], [245, '北汽MPV'], [64, '北汽轿车'], [34, '北汽皮卡'], [1077, '长安轿车'], [1570, '长安SUV'], [291, '长安厢式车'], [310, '长安皮卡'], [91, '长城皮卡'], [1757, '大众轿车'], [826, '大众SUV'], [58, '大众MPV'], [700, '东风SUV'], [633, '东风厢型车'], [136, '东风轿车'], [101, '东风皮卡'], [364, '东南轿车'], [857, '丰田轿车'], [852, '丰田SUV'], [101, '丰田MPV'], [299, '福特轿车'], [320, '福特SUV'], [142, '福特MPV'], [787, '广汽传祺轿车'], [601, '广汽传祺SUV'], [162, '广汽传祺MPV'], [74, '观致汽车'], [1477, '长城SUV'], [163, '海马轿车'], [127, '海马SUV'], [913, '吉利轿车'], [632, '吉利SUV'], [1396, '吉普SUV'], [0, '捷达汽车'], [476, '江淮家用车'], [907, '江淮商用车'], [81, '金杯'], [473, '捷途'], [431, '凯迪拉克轿车'], [329, '凯迪拉克suv'], [191, '雷克萨斯轿车'], [500, '雷克萨斯suv'], [1126, '路虎suv'], [358, '领克'], [316, '铃木'], [148, '雷诺'], [164, '猎豹'], [1148, '马自达轿车'], [464, '名爵'], [153, '纳智捷suv'], [915, '奇瑞轿车'], [1397, '奇瑞suv'], [872, '起亚轿车'], [114, '起亚suv'], [326, '启辰suv'], [633, '日产轿车'], [610, '日产suv'], [290, '日产厢式车'], [970, '荣威'], [838, '斯柯达轿车'], [671, '三菱'], [154, '上汽大通'], [170, '斯巴鲁'], [114, 'SWM斯威汽车'], [45, '特斯拉'], [800, '沃尔沃'], [236, '五菱厢式车'], [20, '五菱皮卡'], [427, 'WEY SUV'], [40, '蔚来汽车'], [631, '现代轿车'], [453, '现代suv'], [2379, '雪佛兰轿车'], [419, '雪佛兰suv'], [955, '雪铁龙'], [0, '新宝骏'], [358, '英菲尼迪'], [237, '天津一汽'], [1051, '众泰'], [1327, '中华'], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, '']]\n",
      "total number of training images 26\n",
      "[[7, '奥迪轿车'], [0, '奥迪SUV'], [6, '奔驰轿车'], [0, '奔驰SUV'], [0, '奔驰厢式车'], [8, '宝马轿车'], [0, '宝马SUV'], [0, '本田轿车'], [5, '本田SUV'], [0, '本田MPV'], [0, '别克轿车'], [0, '别克SUV'], [0, '别克MPV'], [0, '比亚迪轿车'], [0, '比亚迪SUV'], [0, '宝骏'], [0, '标致轿车'], [0, '标致SUV'], [0, '北京轿车'], [0, '北京SUV'], [0, '宝沃SUV'], [0, '奔腾轿车'], [0, '奔腾SUV'], [0, '北汽SUV'], [0, '北汽MPV'], [0, '北汽轿车'], [0, '北汽皮卡'], [0, '长安轿车'], [0, '长安SUV'], [0, '长安厢式车'], [0, '长安皮卡'], [0, '长城皮卡'], [0, '大众轿车'], [0, '大众SUV'], [0, '大众MPV'], [0, '东风SUV'], [0, '东风厢型车'], [0, '东风轿车'], [0, '东风皮卡'], [0, '东南轿车'], [0, '丰田轿车'], [0, '丰田SUV'], [0, '丰田MPV'], [0, '福特轿车'], [0, '福特SUV'], [0, '福特MPV'], [0, '广汽传祺轿车'], [0, '广汽传祺SUV'], [0, '广汽传祺MPV'], [0, '观致汽车'], [0, '长城SUV'], [0, '海马轿车'], [0, '海马SUV'], [0, '吉利轿车'], [0, '吉利SUV'], [0, '吉普SUV'], [0, '捷达汽车'], [0, '江淮家用车'], [0, '江淮商用车'], [0, '金杯'], [0, '捷途'], [0, '凯迪拉克轿车'], [0, '凯迪拉克suv'], [0, '雷克萨斯轿车'], [0, '雷克萨斯suv'], [0, '路虎suv'], [0, '领克'], [0, '铃木'], [0, '雷诺'], [0, '猎豹'], [0, '马自达轿车'], [0, '名爵'], [0, '纳智捷suv'], [0, '奇瑞轿车'], [0, '奇瑞suv'], [0, '起亚轿车'], [0, '起亚suv'], [0, '启辰suv'], [0, '日产轿车'], [0, '日产suv'], [0, '日产厢式车'], [0, '荣威'], [0, '斯柯达轿车'], [0, '三菱'], [0, '上汽大通'], [0, '斯巴鲁'], [0, 'SWM斯威汽车'], [0, '特斯拉'], [0, '沃尔沃'], [0, '五菱厢式车'], [0, '五菱皮卡'], [0, 'WEY SUV'], [0, '蔚来汽车'], [0, '现代轿车'], [0, '现代suv'], [0, '雪佛兰轿车'], [0, '雪佛兰suv'], [0, '雪铁龙'], [0, '新宝骏'], [0, '英菲尼迪'], [0, '天津一汽'], [0, '众泰'], [0, '中华'], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, '']]\n",
      "total number of training images 26\n",
      "[[7, '奥迪轿车'], [0, '奥迪SUV'], [6, '奔驰轿车'], [0, '奔驰SUV'], [0, '奔驰厢式车'], [8, '宝马轿车'], [0, '宝马SUV'], [0, '本田轿车'], [5, '本田SUV'], [0, '本田MPV'], [0, '别克轿车'], [0, '别克SUV'], [0, '别克MPV'], [0, '比亚迪轿车'], [0, '比亚迪SUV'], [0, '宝骏'], [0, '标致轿车'], [0, '标致SUV'], [0, '北京轿车'], [0, '北京SUV'], [0, '宝沃SUV'], [0, '奔腾轿车'], [0, '奔腾SUV'], [0, '北汽SUV'], [0, '北汽MPV'], [0, '北汽轿车'], [0, '北汽皮卡'], [0, '长安轿车'], [0, '长安SUV'], [0, '长安厢式车'], [0, '长安皮卡'], [0, '长城皮卡'], [0, '大众轿车'], [0, '大众SUV'], [0, '大众MPV'], [0, '东风SUV'], [0, '东风厢型车'], [0, '东风轿车'], [0, '东风皮卡'], [0, '东南轿车'], [0, '丰田轿车'], [0, '丰田SUV'], [0, '丰田MPV'], [0, '福特轿车'], [0, '福特SUV'], [0, '福特MPV'], [0, '广汽传祺轿车'], [0, '广汽传祺SUV'], [0, '广汽传祺MPV'], [0, '观致汽车'], [0, '长城SUV'], [0, '海马轿车'], [0, '海马SUV'], [0, '吉利轿车'], [0, '吉利SUV'], [0, '吉普SUV'], [0, '捷达汽车'], [0, '江淮家用车'], [0, '江淮商用车'], [0, '金杯'], [0, '捷途'], [0, '凯迪拉克轿车'], [0, '凯迪拉克suv'], [0, '雷克萨斯轿车'], [0, '雷克萨斯suv'], [0, '路虎suv'], [0, '领克'], [0, '铃木'], [0, '雷诺'], [0, '猎豹'], [0, '马自达轿车'], [0, '名爵'], [0, '纳智捷suv'], [0, '奇瑞轿车'], [0, '奇瑞suv'], [0, '起亚轿车'], [0, '起亚suv'], [0, '启辰suv'], [0, '日产轿车'], [0, '日产suv'], [0, '日产厢式车'], [0, '荣威'], [0, '斯柯达轿车'], [0, '三菱'], [0, '上汽大通'], [0, '斯巴鲁'], [0, 'SWM斯威汽车'], [0, '特斯拉'], [0, '沃尔沃'], [0, '五菱厢式车'], [0, '五菱皮卡'], [0, 'WEY SUV'], [0, '蔚来汽车'], [0, '现代轿车'], [0, '现代suv'], [0, '雪佛兰轿车'], [0, '雪佛兰suv'], [0, '雪铁龙'], [0, '新宝骏'], [0, '英菲尼迪'], [0, '天津一汽'], [0, '众泰'], [0, '中华'], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, ''], [0, '']]\n"
     ]
    }
   ],
   "source": [
    "train_set = DrivingDataset(data_dir=\"C:/Users/zhaoh/Downloads/car images/\", is_train=True,transform=transform) \n",
    "val_set = DrivingDataset(data_dir=\"C:/Users/zhaoh/Downloads/test image/\", is_train=True,transform=transform) \n",
    "test_set = DrivingDataset(data_dir=\"C:/Users/zhaoh/Downloads/test image/\", is_train=True,transform=transform) \n",
    "#756 1133 102 868 1367 790"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "#n_workers = multiprocessing.cpu_count()\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "valloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i=0\\nfor index, [image_batch, label] in enumerate(trainloader):#next(iter(trainloader))\\n        #print(image_batch[index].shape)\\n        #print(label.shape,index,image_batch.shape)\\n        \\n        for j in range(batch_size):\\n        #print(image_batch[i])\\n            if label[j]==0:\\n                img = np.moveaxis((image_batch[j].numpy()+2)/4,0,2)\\n                i+=1\\n                #print(i)\\n                plt.subplot(2,4,(i%8+1))\\n                plt.imshow(img)\\n                if i%8==0:\\n                    plt.show()\\n            \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''i=0\n",
    "for index, [image_batch, label] in enumerate(trainloader):#next(iter(trainloader))\n",
    "        #print(image_batch[index].shape)\n",
    "        #print(label.shape,index,image_batch.shape)\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "        #print(image_batch[i])\n",
    "            if label[j]==0:\n",
    "                img = np.moveaxis((image_batch[j].numpy()+2)/4,0,2)\n",
    "                i+=1\n",
    "                #print(i)\n",
    "                plt.subplot(2,4,(i%8+1))\n",
    "                plt.imshow(img)\n",
    "                if i%8==0:\n",
    "                    plt.show()\n",
    "            \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the network below,resnet 152\n",
    "####################################\n",
    "#######################################\n",
    "###########################################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.hub import load_state_dict_from_url \n",
    "\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
    "           'wide_resnet50_2', 'wide_resnet101_2']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
    "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
    "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
    "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    __constants__ = ['downsample']\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    __constants__ = ['downsample']\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,#########################################################\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # Allow for accessing forward method in a inherited class\n",
    "    forward = _forward\n",
    "\n",
    "\n",
    "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def resnet152(pretrained=True, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-152 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (12): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (13): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (14): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (15): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (16): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (17): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (18): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (19): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (20): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (21): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (22): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (23): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (24): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (25): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (26): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (27): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (28): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (29): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (30): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (31): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (32): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (33): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (34): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (35): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=120, bias=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "model = resnet152(pretrained=True)#torch.load('model2.pkl')\n",
    "\n",
    "fc_features = model.fc.in_features\n",
    "\n",
    "model.fc = nn.Linear(fc_features, 120)\n",
    "\n",
    "#criterion = FocalLoss()\n",
    "criterion_class= nn.CrossEntropyLoss() \n",
    "criterion_hinge=nn.MultiMarginLoss()\n",
    "\n",
    "learning_rate = 1e-4\n",
    "print(model)\n",
    "model=model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\") # 这里是“欧一”，不是“零一”\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_epoch(model, train_loader, criterion1,criterion2, optimizer):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total_predictions = 0.0\n",
    "    correct_predictions = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for data, target in tqdm(train_loader):  \n",
    "        #print(target)\n",
    "        optimizer.zero_grad()   \n",
    "        data = data.to(device)\n",
    "        target = target.long().to(device)\n",
    "        target=target.squeeze(1)#############\n",
    "        outputs = model(data)\n",
    "        loss = criterion1(outputs, target)####################\n",
    "        loss+=criterion2(outputs, target)\n",
    "        print(loss)\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_predictions += target.size(0)\n",
    "        correct_predictions += (predicted == target).sum().item()\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "            #loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    running_loss /= len(train_loader)\n",
    "    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time,' train accuracy',acc)\n",
    "    return running_loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion1,criterion2):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "        Predicted=[]\n",
    "        for data, target in test_loader: \n",
    "            #print(data.shape,target.shape)\n",
    "            data = data.to(device) \n",
    "            target = target.long().to(device)\n",
    "            target=target.squeeze(1)##################\n",
    "            outputs = model(data)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            Predicted.append(predicted)\n",
    "            print('target',target,'prediction',predicted)\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "            loss = criterion1(outputs, target)####################\n",
    "            loss+=criterion2(outputs, target)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "        running_loss /= len(test_loader)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        print('Testing Loss: ', running_loss)\n",
    "        print('Testing Accuracy: ', acc, '%')\n",
    "        \n",
    "        return running_loss, acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('parameter.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                         | 0/4890 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.9155, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                               | 1/4890 [00:00<1:17:39,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.8567, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                               | 2/4890 [00:01<1:14:03,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.8807, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                               | 3/4890 [00:02<1:11:31,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.7227, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                               | 4/4890 [00:03<1:10:06,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.6502, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                               | 5/4890 [00:04<1:09:46,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.4168, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                               | 6/4890 [00:05<1:10:21,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.7637, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                               | 7/4890 [00:05<1:09:21,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.0192, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▏                                                                              | 8/4890 [00:06<1:09:18,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5151, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▏                                                                              | 9/4890 [00:07<1:07:46,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.2197, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▏                                                                             | 10/4890 [00:08<1:09:34,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.3140, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▏                                                                             | 11/4890 [00:09<1:09:24,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.2334, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▏                                                                             | 12/4890 [00:10<1:08:10,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.9528, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▏                                                                             | 13/4890 [00:11<1:10:02,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.2951, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▏                                                                             | 14/4890 [00:11<1:07:57,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.6537, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▏                                                                             | 15/4890 [00:12<1:07:32,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.3050, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▎                                                                             | 16/4890 [00:13<1:05:51,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.1927, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▎                                                                             | 17/4890 [00:14<1:04:49,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.3177, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▎                                                                             | 18/4890 [00:15<1:05:59,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.1823, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▎                                                                             | 19/4890 [00:15<1:08:55,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.9273, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▎                                                                             | 20/4890 [00:16<1:12:32,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.6196, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▎                                                                             | 21/4890 [00:17<1:11:05,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.5244, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▎                                                                             | 22/4890 [00:18<1:09:05,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.4909, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▎                                                                             | 23/4890 [00:19<1:13:12,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.2655, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▍                                                                             | 24/4890 [00:20<1:12:45,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.7392, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▍                                                                             | 25/4890 [00:21<1:11:25,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.9280, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▍                                                                             | 26/4890 [00:22<1:11:19,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.1530, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▍                                                                             | 27/4890 [00:23<1:11:21,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.9327, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▍                                                                             | 28/4890 [00:23<1:09:55,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.6266, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▍                                                                             | 29/4890 [00:24<1:10:42,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.9897, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▍                                                                             | 30/4890 [00:25<1:09:05,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.3525, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▍                                                                             | 31/4890 [00:26<1:08:48,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.9933, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▌                                                                             | 32/4890 [00:27<1:09:29,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.5629, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▌                                                                             | 33/4890 [00:28<1:07:40,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.1176, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▌                                                                             | 34/4890 [00:28<1:06:41,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.8990, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▌                                                                             | 35/4890 [00:29<1:06:53,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.7853, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▌                                                                             | 36/4890 [00:30<1:07:04,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.1873, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▌                                                                             | 37/4890 [00:31<1:08:01,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.5263, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▌                                                                             | 38/4890 [00:32<1:07:26,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.1852, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▌                                                                             | 39/4890 [00:33<1:06:43,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.7437, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▋                                                                             | 40/4890 [00:34<1:10:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.9980, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▋                                                                             | 41/4890 [00:34<1:11:42,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.9197, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▋                                                                             | 42/4890 [00:35<1:10:14,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.8555, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▋                                                                             | 43/4890 [00:36<1:08:18,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.2137, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▋                                                                             | 44/4890 [00:37<1:07:29,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.5557, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▋                                                                             | 45/4890 [00:38<1:05:53,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.0732, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▋                                                                             | 46/4890 [00:39<1:07:46,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3180, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▋                                                                             | 47/4890 [00:39<1:08:57,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.5375, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▊                                                                             | 48/4890 [00:40<1:07:25,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.4784, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▊                                                                             | 49/4890 [00:41<1:07:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3771, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▊                                                                             | 50/4890 [00:42<1:07:56,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.1397, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▊                                                                             | 51/4890 [00:43<1:06:45,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.7203, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▊                                                                             | 52/4890 [00:44<1:07:16,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6129, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▊                                                                             | 53/4890 [00:44<1:07:07,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.1674, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▊                                                                             | 54/4890 [00:45<1:05:56,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.9886, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▉                                                                             | 55/4890 [00:46<1:05:40,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.8061, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▉                                                                             | 56/4890 [00:47<1:06:05,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.7777, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▉                                                                             | 57/4890 [00:48<1:06:34,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6079, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▉                                                                             | 58/4890 [00:48<1:05:44,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.2183, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▉                                                                             | 59/4890 [00:49<1:07:38,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.0388, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▉                                                                             | 60/4890 [00:50<1:07:07,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.8150, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▉                                                                             | 61/4890 [00:51<1:05:52,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3170, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-160e81c904d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mTrain_acc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_hinge\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_hinge\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#val_loss, val_acc = test_model(model, valloader, criterion)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-2893c767d3ce>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(model, train_loader, criterion1, criterion2, optimizer)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mscaled_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;31m#loss.backward()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\apex-0.1-py3.7.egg\\apex\\amp\\_initialize.py\u001b[0m in \u001b[0;36mnew_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[1;32mdef\u001b[0m \u001b[0mnew_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m                     \u001b[1;32mwith\u001b[0m \u001b[0mdisable_casts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m                         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mold_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mnew_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     99\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\apex-0.1-py3.7.egg\\apex\\amp\\wrap.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(arg0, *args, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_tensor_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_amp_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_active\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0morig_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'HalfTensor'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\apex-0.1-py3.7.egg\\apex\\amp\\wrap.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(arg0, *args, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_tensor_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_amp_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_active\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0morig_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'HalfTensor'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "n_epochs = 5\n",
    "Train_loss = []\n",
    "Test_loss = []\n",
    "Test_acc = []\n",
    "Val_loss = []\n",
    "Val_acc = []\n",
    "Train_acc=[]\n",
    "for i in range(n_epochs):\n",
    "    train_loss,train_acc = train_epoch(model, trainloader, criterion_class, criterion_hinge,optimizer)\n",
    "    test_loss, test_acc = test_model(model, testloader, criterion_class, criterion_hinge)\n",
    "    #val_loss, val_acc = test_model(model, valloader, criterion)\n",
    "    Train_acc.append(train_acc)\n",
    "    Train_loss.append(train_loss)\n",
    "    #Val_acc.append(val_acc)\n",
    "    #Val_loss.append(val_loss)\n",
    "    Test_loss.append(test_loss)\n",
    "    Test_acc.append(test_acc)\n",
    "    print(i,'epoch='*20)\n",
    "#test_loss, test_acc = test_model(model, testloader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'parameter.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = test_model(model, testloader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path='C:/Users/zhaoh/Downloads/test image/'\n",
    "for f in fs.open_fs(test_path).walk.files(filter=[\"*.jpg\"]):\n",
    "    name=os.path.join(test_path+str(f))\n",
    "    print(name)\n",
    "        #test_path =name\n",
    "        #newname=os.path.join(test_path,'benz suv'+str(i)+'.jpg')\n",
    "        #os.rename(name, newname)\n",
    "        #test_image=newname\n",
    "    filename = name\n",
    "    \n",
    "    img=io.imread(filename)\n",
    "    \n",
    "    img=cv2.resize(img,(224,224))\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    outputs = model(np.reshape(torch.FloatTensor(img),(1,3,224,224)).to(device))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    print(outputs)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
